{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wavemx/AI_spectogram/blob/master/Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D5e6--_GM5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://mpp0xc0ae45ef.blob.core.windows.net/drivendata-mpp-storage/data/19/public/data-release.zip\n",
        "\n",
        "!unzip -o data-release.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD2iSNmQVb0Y",
        "colab_type": "text"
      },
      "source": [
        "# **ALL in One**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGADvN7QSLFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical \n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.core import Dropout\n",
        "from keras.optimizers import rmsprop\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "\n",
        "#Read Data\n",
        "train_df = pd.read_csv('train_labels.csv', index_col=0)\n",
        "\n",
        "train_df['current_file'] = train_df.index.map(lambda id: f'train/{id}_c.png')\n",
        "train_df['voltage_file'] = train_df.index.map(lambda id: f'train/{id}_v.png')\n",
        "\n",
        "print(train_df.head())\n",
        "\n",
        "#Helper Function\n",
        "\n",
        "def read_spectograms(file_paths, img_rows, img_cols, as_gray, channels):\n",
        "  \"\"\"\n",
        "  Reads the spectogram files from disk and normalizes the pixel values\n",
        "    @params:\n",
        "      file_paths - Array of file paths to read from\n",
        "      img_rows - The image height.\n",
        "      img_cols - The image width.\n",
        "      as_grey - Read the image as Greyscale or RGB.\n",
        "      channels - Number of channels.\n",
        "    @returns:\n",
        "      The created and compiled model (Model)        \n",
        "  \"\"\"\n",
        "  images = []\n",
        "  \n",
        "  for file_path in file_paths:\n",
        "        images.append(imread(file_path, as_gray = as_gray))\n",
        "        \n",
        "        images = np.asarray(images, dtype=np.float32)\n",
        "  \n",
        "  # normalize\n",
        "        images = images / np.max(images)\n",
        "  \n",
        "  # reshape to match Keras expectaions\n",
        "        images = images.reshape(images.shape[0], img_rows, img_cols, channels)\n",
        "        return images\n",
        "\n",
        "#Parameters\n",
        "\n",
        "as_gray = True\n",
        "in_channel = 4\n",
        "\n",
        "if as_gray:\n",
        "    in_channel = 1\n",
        "\n",
        "img_rows, img_cols = 176, 128\n",
        "num_classes = 11 # number of appliances\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "input_shape = (img_rows, img_cols, in_channel)\n",
        "input_img = Input(shape = input_shape)\n",
        "\n",
        "#Current files\n",
        "\n",
        "x_train_current = read_spectograms(train_df.current_file.values, img_rows, img_cols, as_gray, in_channel)\n",
        "\n",
        "#Voltage files\n",
        "\n",
        "x_train_voltage = read_spectograms(train_df.voltage_file.values, img_rows, img_cols, as_gray, in_channel)\n",
        "\n",
        "#Labels\n",
        "\n",
        "labels = train_df.appliance.values\n",
        "\n",
        "# convert class vectors to binary class matrices One Hot Encoding\n",
        "labels = keras.utils.to_categorical(labels, num_classes)\n",
        "\n",
        "#Show Data\n",
        "appliances = [\n",
        "    'Compact Fluorescent Lamp', \n",
        "    'Hairdryer', \n",
        "    'Microwave', \n",
        "    'Air Conditioner', \n",
        "    'Fridge', \n",
        "    'Laptop', \n",
        "    'Vacuum', \n",
        "    'Incandescent Light Bulb', \n",
        "    'Fan',\n",
        "    'Washing Machine',\n",
        "    'Heater'\n",
        "]\n",
        "\n",
        "# pick a random index from the list\n",
        "rn_appliance = np.random.choice(train_df.appliance.values)\n",
        "rn_label = train_df.appliance.values[rn_appliance]\n",
        "rn_current = x_train_current[rn_appliance]\n",
        "rn_voltage = x_train_voltage[rn_appliance]\n",
        "\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "\n",
        "plt.suptitle(f\"{appliances[rn_label]} (Label: {rn_label})\", fontsize=\"x-large\")\n",
        "\n",
        "plt.subplot(121)\n",
        "curr_img = None\n",
        "if as_gray:\n",
        "    curr_img = np.reshape(rn_current, (img_rows, img_cols))\n",
        "else:\n",
        "    curr_img = np.reshape(rn_current, (img_rows, img_cols, in_channel))\n",
        "\n",
        "plt.imshow(curr_img, cmap='gray')\n",
        "plt.title(\"Current\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "plt.subplot(122)\n",
        "if as_gray:\n",
        "    curr_img = np.reshape(rn_voltage, (img_rows, img_cols))\n",
        "else:\n",
        "    curr_img = np.reshape(rn_voltage, (img_rows, img_cols, in_channel))\n",
        "\n",
        "plt.imshow(curr_img, cmap='gray')\n",
        "plt.title(\"Voltage\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "#Create and train the model\n",
        "#Split in train and test batches\n",
        "\n",
        "x_train_comp = np.stack((x_train_current, x_train_voltage), axis=4)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train_comp, labels, test_size = 0.4, random_state=666)\n",
        "\n",
        "# take them apart\n",
        "x_train_current = x_train[:,:,:,:,0]\n",
        "x_test_current = x_test[:,:,:,:,0]\n",
        "\n",
        "x_train_voltage = x_train[:,:,:,:,1]\n",
        "x_test_voltage = x_test[:,:,:,:,1]\n",
        "\n",
        "#Create Model\n",
        "def create_convolution_layers(input_img):\n",
        "  model = Conv2D(32, (3, 3), padding='same', input_shape=input_shape)(input_img)\n",
        "  model = LeakyReLU(alpha=0.1)(model)\n",
        "  model = MaxPooling2D((2, 2),padding='same')(model)\n",
        "  model = Dropout(0.25)(model)\n",
        "  \n",
        "  model = Conv2D(64, (3, 3), padding='same')(model)\n",
        "  model = LeakyReLU(alpha=0.1)(model)\n",
        "  model = MaxPooling2D(pool_size=(2, 2),padding='same')(model)\n",
        "  model = Dropout(0.25)(model)\n",
        "    \n",
        "  model = Conv2D(128, (3, 3), padding='same')(model)\n",
        "  model = LeakyReLU(alpha=0.1)(model)\n",
        "  model = MaxPooling2D(pool_size=(2, 2),padding='same')(model)\n",
        "  model = Dropout(0.4)(model)\n",
        "    \n",
        "  return model\n",
        "current_input = Input(shape=input_shape)\n",
        "current_model = create_convolution_layers(current_input)\n",
        "\n",
        "voltage_input = Input(shape=input_shape)\n",
        "voltage_model = create_convolution_layers(voltage_input)\n",
        "\n",
        "conv = concatenate([current_model, voltage_model])\n",
        "\n",
        "conv = Flatten()(conv)\n",
        "\n",
        "dense = Dense(512)(conv)\n",
        "dense = LeakyReLU(alpha=0.1)(dense)\n",
        "dense = Dropout(0.5)(dense)\n",
        "\n",
        "output = Dense(num_classes, activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=[current_input, voltage_input], outputs=[output])\n",
        "\n",
        "opt = optimizers.Adam()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "#Train Model\n",
        "\n",
        "best_weights_file=\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(best_weights_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "#lr_reducer = ReduceLROnPlateau(verbose=1)\n",
        "\n",
        "callbacks = [checkpoint]\n",
        "\n",
        "model.fit([x_train_current, x_train_voltage], y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=callbacks,\n",
        "          verbose=1,\n",
        "          validation_data=([x_test_current, x_test_voltage], y_test),\n",
        "          shuffle=True)\n",
        "          \n",
        "\n",
        "#Basic Evaluation\n",
        "\n",
        "# load weights\n",
        "model.load_weights(best_weights_file)\n",
        "\n",
        "final_loss, final_acc = model.evaluate([x_test_current, x_test_voltage], y_test, verbose=1)\n",
        "print(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(final_loss, final_acc))\n",
        "\n",
        "#Prediction\n",
        "\n",
        "predict_df = pd.read_csv('submission_format.csv', index_col=0)\n",
        "\n",
        "predict_df['current_file'] = predict_df.index.map(lambda id: f'test/{id}_c.png')\n",
        "predict_df['voltage_file'] = predict_df.index.map(lambda id: f'test/{id}_v.png')\n",
        "\n",
        "x_test_current = read_spectograms(predict_df.current_file.values, img_rows, img_cols, as_gray, in_channel)\n",
        "x_test_voltage = read_spectograms(predict_df.voltage_file.values, img_rows, img_cols, as_gray, in_channel)\n",
        "\n",
        "\n",
        "#get the predictions for the test data\n",
        "\n",
        "predicted_classes = model.predict([x_test_current, x_test_voltage])\n",
        "\n",
        "predict_df.appliance = np.argmax(predicted_classes,axis=1)\n",
        "\n",
        "predict_df = predict_df.drop(['current_file', 'voltage_file'], axis=1)\n",
        "\n",
        "predict_df.to_csv('submission.csv')\n",
        "\n",
        "#from google.colab import files\n",
        "\n",
        "#files.download('submission.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKAFVAr6sGAJ",
        "colab_type": "text"
      },
      "source": [
        "# **NEW CODE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJOqqA9isRKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bc83f603-d2ca-4f1b-abe0-95b31a0c2191"
      },
      "source": [
        "\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"\n",
        "Prepares data for easier processing by the neural net classifier.\n",
        "\"\"\"\n",
        "import logging\n",
        "\n",
        "\n",
        "import math\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "#from PIL import Image\n",
        "\n",
        "\n",
        "__author__ = 'Greg Gasowski'\n",
        "__copyright__ = 'Copyright 2019, WaveMax LLC'\n",
        "__credits__ = ['Greg Gasowski']\n",
        "__license__ = 'MIT'\n",
        "__version__ = '1.0'\n",
        "__maintainer__ = 'Greg Gasowski'\n",
        "__email__ = 'gregory.gasowski@gmail.com'\n",
        "__status__ = 'Production'\n",
        "\n",
        "#Configuration/parameters\n",
        "classes_count = 11\n",
        "image_width = (2*118)//1\n",
        "image_height = 128//1\n",
        "image_data_size = image_width*image_height\n",
        "channels = 1\n",
        "\n",
        "data_root_path = './data/'\n",
        "\n",
        "input_data_file_path = data_root_path + 'data-release.zip'\n",
        "\n",
        "training_image_data_file_path = data_root_path + 'image_train.data'\n",
        "training_labels_data_file_path = data_root_path + 'image_train_labels.csv'\n",
        "testing_data_file_path = data_root_path + 'image_test.data'\n",
        "\n",
        "testing_submission_file_path = data_root_path + 'submission_format.csv'\n",
        "\n",
        "def compose_train_image(p_img1, p_img2) :\n",
        "    \"\"\"\n",
        "    Creates a horizontally stacked image using two input images\n",
        "\n",
        "    @params:\n",
        "        p_img1 - Required : first image source (Image)\n",
        "        p_img2 - Required : second image source (Image) \n",
        "\n",
        "    @returns:\n",
        "        The stacked image (Image)    \n",
        "    \"\"\" \n",
        "\n",
        "    #Stacks images horizontally (i.e. one afer another on width axis)\n",
        "    img_merge_data = np.hstack([np.asarray(p_img1), np.asarray(p_img2)])\n",
        "    img_merge = Image.fromarray( img_merge_data )\n",
        "        \n",
        "    return img_merge\n",
        "\n",
        "def get_image_data(p_image) :\n",
        "    \"\"\"\n",
        "    Returns a flatten array of image pixel values (1 channel gray pallete)\n",
        "\n",
        "    @params:\n",
        "        p_image - Required : the input image (Image)\n",
        "\n",
        "    @returns:\n",
        "        Flattened array of image data (array)\n",
        "    \"\"\"  \n",
        "\n",
        "    #Generates image data from the received image object\n",
        "    width, height = p_image.size\n",
        "    data = np.asarray(p_image).reshape(height*width)\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "def create_trainining_images_data_file(p_input_data_file_path, p_training_data_file_path):\n",
        "    \"\"\"\n",
        "    Creates training information data\n",
        "\n",
        "    @params:\n",
        "        p_input_data_file_path - Required: the input data file path (String)\n",
        "        p_training_data_file_path - Required: the output training data file path (String)\n",
        "\n",
        "    @returns:\n",
        "        The extracted training labels (array)\n",
        "    \"\"\"  \n",
        "\n",
        "    training_labels_file_path = 'train_labels.csv'\n",
        "    \n",
        "    labels = None\n",
        "\n",
        "    with open(p_training_data_file_path, 'w+b') as data_file :\n",
        "        with ZipFile(p_input_data_file_path) as data_zip:\n",
        "            with data_zip.open(training_labels_file_path) as train_labels_file:\n",
        "                content = train_labels_file.read()\n",
        "                with BytesIO(content) as io_content:\n",
        "                    train_labels = pd.read_csv(io_content)\n",
        "\n",
        "                    max_count = train_labels.shape[0]    \n",
        "                    labels = np.zeros(max_count)\n",
        "\n",
        "                    count = 0\n",
        "\n",
        "                    for _, row in train_labels.iterrows() :\n",
        "\n",
        "                        with data_zip.open('train/' + str(row[\"id\"]) + \"_c.png\") as c_file :\n",
        "                            with BytesIO(c_file.read()) as input_buffer:\n",
        "                                c_image = Image.open(input_buffer).convert(\"L\")\n",
        "\n",
        "                        with data_zip.open('train/' + str(row[\"id\"]) + \"_v.png\") as v_file :\n",
        "                            with BytesIO(v_file.read()) as input_buffer:\n",
        "                                v_image = Image.open(input_buffer).convert(\"L\")\n",
        "\n",
        "                        image_data = get_image_data(compose_train_image(c_image, v_image))\n",
        "\n",
        "                        labels[count] = row[\"appliance\"]\n",
        "                        data_file.write(image_data)\n",
        "\n",
        "                        count = count + 1       \n",
        "\n",
        "    return labels[:count]\n",
        "\n",
        "def create_training_labels(p_labels, p_labels_data_file_path) :\n",
        "    \"\"\"\n",
        "    Writes the training labels to a destination file\n",
        "\n",
        "    @params:\n",
        "        p_labels - Required: the array of labels (array)\n",
        "    \"\"\" \n",
        "\n",
        "    classes = pd.DataFrame(p_labels.astype(int))\n",
        "    classes.to_csv(p_labels_data_file_path, header=None)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def create_testing_images_data_file(p_input_data_file_path, p_testing_data_file_path):\n",
        "    \"\"\"\n",
        "    Creates testing information data\n",
        "\n",
        "    @params:\n",
        "        p_input_data_file_path - Required: the input data file path (String)\n",
        "        p_testing_data_file_path - Required: the output testing data file path (String)\n",
        "\n",
        "    @returns:\n",
        "        The count of test images (int)\n",
        "    \"\"\"  \n",
        "\n",
        "    submission_format_file_path = 'submission_format.csv'\n",
        "\n",
        "    with open(p_testing_data_file_path, 'w+b') as data_file :\n",
        "        with ZipFile(p_input_data_file_path) as data_zip:\n",
        "            with data_zip.open(submission_format_file_path) as submission_format_file:\n",
        "                content = submission_format_file.read()\n",
        "                with BytesIO(content) as io_content:\n",
        "                    submission_indexes = pd.read_csv(io_content)\n",
        "\n",
        "                    count = 0\n",
        "\n",
        "                    for _, row in submission_indexes.iterrows() :\n",
        "\n",
        "                        with data_zip.open('test/' + str(row[\"id\"]) + \"_c.png\") as c_file :\n",
        "                            with BytesIO(c_file.read()) as input_buffer:\n",
        "                                c_image = Image.open(input_buffer).convert(\"L\")\n",
        "\n",
        "                        with data_zip.open('test/' + str(row[\"id\"]) + \"_v.png\") as v_file :\n",
        "                            with BytesIO(v_file.read()) as input_buffer:\n",
        "                                v_image = Image.open(input_buffer).convert(\"L\")\n",
        "\n",
        "                        image_data = get_image_data(compose_train_image(c_image, v_image))\n",
        "                        data_file.write(image_data)\n",
        "\n",
        "                        count = count + 1       \n",
        "\n",
        "    return count\n",
        "\n",
        "def create_testing_submission(p_input_data_file_path, p_testing_submission_file_path) :\n",
        "    \"\"\"\n",
        "    Writes the submission data to a destination file\n",
        "\n",
        "    @params:\n",
        "        p_input_data_file_path - Required: the input data file path (String)\n",
        "        p_testing_submission_file_path - Required: the testing submission file path (String)\n",
        "    \"\"\" \n",
        "    submission_format_file_path = 'submission_format.csv'\n",
        "\n",
        "    with ZipFile(p_input_data_file_path) as data_zip:\n",
        "        with data_zip.open(submission_format_file_path) as submission_format_file:\n",
        "            content = submission_format_file.read()\n",
        "            with BytesIO(content) as io_content:\n",
        "                submission_indexes = pd.read_csv(io_content)\n",
        "                submission_indexes.to_csv(p_testing_submission_file_path, index=False)\n",
        "\n",
        "    return\n",
        "\n",
        "def main() :\n",
        "    \"\"\"\n",
        "    Entry point\n",
        "    \"\"\"\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    \n",
        "    #create training data\n",
        "    logging.info('Creating training data ...')\n",
        "    training_labels  = create_trainining_images_data_file(input_data_file_path, training_image_data_file_path)\n",
        "    create_training_labels(training_labels, training_labels_data_file_path)\n",
        "    logging.info(\"Processed training images count: %d\" % training_labels.shape[0])\n",
        "    logging.info('Creating training data DONE')\n",
        "\n",
        "\n",
        "    logging.info('Creating testing data ...')\n",
        "    testing_count = create_testing_images_data_file(input_data_file_path, testing_data_file_path)\n",
        "    create_testing_submission(input_data_file_path, testing_submission_file_path)\n",
        "    logging.info(\"Processed testing images count: %d\" % testing_count)\n",
        "    logging.info('Creating testing data DONE')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:Creating training data ...\n",
            "INFO:root:Processed training images count: 576\n",
            "INFO:root:Creating training data DONE\n",
            "INFO:root:Creating testing data ...\n",
            "INFO:root:Processed testing images count: 384\n",
            "INFO:root:Creating testing data DONE\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAOX83vssikC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Classifies the test data and generates the submissions.\n",
        "\"\"\"\n",
        "import logging\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import keras.utils.np_utils as utl\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from PIL import Image, ImageFilter\n",
        "from tensorflow import set_random_seed\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.core import Dropout\n",
        "from keras.optimizers import rmsprop\n",
        "from keras import regularizers\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zubfYdV1sEvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "__author__ = 'Greg Gasowski'\n",
        "__copyright__ = 'Copyright 2019, WaveMax LLC'\n",
        "__credits__ = ['Greg Gasowski']\n",
        "__license__ = 'MIT'\n",
        "__version__ = '1.0'\n",
        "__maintainer__ = 'Greg Gasowski'\n",
        "__email__ = 'gregory.gasowski@gmail.com'\n",
        "__status__ = 'Production'\n",
        "\n",
        "#configuration data \n",
        "image_height = 128//1\n",
        "image_width = 176//1\n",
        "#image_width = 2*118//1\n",
        "training_image_count = 576\n",
        "testing_image_count = 384\n",
        "classes_count = 11\n",
        "\n",
        "data_root_path = './data/'\n",
        "\n",
        "training_image_data_file_path = data_root_path + 'image_train.data'\n",
        "training_labels_data_file_path = data_root_path + 'image_train_labels.csv'\n",
        "testing_data_file_path = data_root_path + 'image_test.data'\n",
        "\n",
        "testing_submission_file_path = data_root_path + 'submission_format.csv'\n",
        "submission_results_file_path =  data_root_path + 'submission_results.csv'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRnwnUvHtXAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_image(p_image_data_file_path, p_position, p_image_width, p_image_height) :\n",
        "    \"\"\"\n",
        "    Reads an image from an image data from a image data repository @see prepare_data.py\n",
        "\n",
        "    @params:\n",
        "        p_image_data_file_path - Required : the image data file path (String)\n",
        "        p_position - Required : second image source (int)\n",
        "        p_image_width - Required: the image width (int)\n",
        "        p_image_height - Required: the image height (int)\n",
        "    \n",
        "    @returns:\n",
        "        The image data (array)    \n",
        "    \"\"\"  \n",
        "    with open(p_image_data_file_path, \"rb\") as image_file :\n",
        "        image_file.seek(p_position * p_image_height* p_image_width)\n",
        "        data = image_file.read(p_image_height * p_image_width)\n",
        "    \n",
        "        data_b = np.frombuffer(data, dtype=np.uint8)\n",
        "\n",
        "    return np.asarray(data_b)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVApx-nMta_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_images(p_images, p_image_width, p_image_height) :\n",
        "    \"\"\"\n",
        "    Processes a set of images so it can be classified by the neurals network model\n",
        "\n",
        "    @params:\n",
        "        p_images - Required : the images to process (String)\n",
        "        p_image_width - Required: the image width (int)\n",
        "        p_image_height - Required: the image height (int)\n",
        "    \"\"\"  \n",
        "    #reshape according to inputs accepted by a Conv2d layer\n",
        "    processed_images = p_images.reshape(p_images.shape[0], p_image_height, p_image_width, 1)\n",
        "\n",
        "    #data normalization to max value (0-255 grayscale values)\n",
        "    processed_images = (processed_images * 1.0) /255\n",
        " \n",
        "    return processed_images\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuhWKrObtmUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_labels(p_labels_file_path) :\n",
        "    \"\"\"\n",
        "    Reads the extracted training labels @see prepare_data.py\n",
        "\n",
        "    @params:\n",
        "        p_labels_file_path - Required : the data file path (String)\n",
        "    @returns:\n",
        "        A dataframe containing the read labels with the column [id] for ordinal id and [label] for the label value    \n",
        "    \"\"\" \n",
        "    labels = pd.read_csv(p_labels_file_path, header= None)\n",
        "    labels.columns = [\"id\", \"label\"]\n",
        "  \n",
        "    return labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WseiaEsGtraS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_labels(p_labels) :\n",
        "    \"\"\"\n",
        "    Processes the read labels\n",
        "\n",
        "    @params:\n",
        "        p_labels - Required: the read labels (array)\n",
        "    @returns:\n",
        "        The processed labels (binarization - one hot-encoded)    \n",
        "    \"\"\"\n",
        "    processed_labels = LabelBinarizer().fit_transform(p_labels)\n",
        "    \n",
        "    return processed_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX0bZufDtvSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   def generate_train_set(\n",
        "                            p_image_training_data_file_path, \n",
        "                            p_labels_file_path, \n",
        "                            p_train_set_size, \n",
        "                            p_image_width, \n",
        "                            p_image_height\n",
        "                        ) :\n",
        "                            \"\"\"\n",
        "                            Generates the training data set\n",
        "\n",
        "                            @params:\n",
        "                                p_image_training_data_file_path - Required: the training image data file path (String)\n",
        "                                p_labels_file_path - Required: the labels file path (String)\n",
        "                                p_train_set_size - Required: the size of the training set (int)\n",
        "                                p_image_width - Required: the image width (int)\n",
        "                                p_image_height - Required: the image height (int)\n",
        "\n",
        "                            @returns:\n",
        "                                (train_labels_processed, train_images_processed) tuple wiht the the processed train labels (array) \n",
        "                                and the processed train images (array)\n",
        "                            \"\"\"\n",
        "                            labels = read_labels(p_labels_file_path)\n",
        "\n",
        "                            labels_batch = np.zeros(p_train_set_size)\n",
        "                            labels_batch = labels[\"label\"][0:p_train_set_size].values\n",
        "\n",
        "                            images_batch = []\n",
        "\n",
        "                            for i in range(0, p_train_set_size) :\n",
        "                                image_data = read_image(p_image_training_data_file_path, i, p_image_width, p_image_height)\n",
        "                                images_batch.append(image_data.reshape(p_image_height, p_image_width))\n",
        "                                #images_batch.append(image_data.reshape(p_image_height, p_image_width))\n",
        "\n",
        "                            train_labels_processed = process_labels(labels_batch)\n",
        "\n",
        "                            train_images_processed = process_images(np.array(images_batch), p_image_width, p_image_height)\n",
        "\n",
        "                            return train_labels_processed, train_images_processed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPXOeBVDt0V0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_test_set(\n",
        "    p_test_image_data_file_path, \n",
        "    p_test_set_size, \n",
        "    p_image_width, \n",
        "    p_image_height\n",
        ") :\n",
        "    \"\"\"\n",
        "    Generates the test data set\n",
        "\n",
        "    @params:\n",
        "        p_test_image_data_file_path - Required: the testing image data file path (String)\n",
        "        p_test_set_size - Required: the size of the testing set (int)\n",
        "        p_image_width - Required: the image width (int)\n",
        "        p_image_height - Required: the image height (int)\n",
        "\n",
        "    @returns:\n",
        "        test_images_processed the processed test images (array)\n",
        "    \"\"\"\n",
        "    images_batch = []\n",
        "\n",
        "    for i in range(0, p_test_set_size) :\n",
        "        image_data = read_image(p_test_image_data_file_path, i, p_image_width, p_image_height)\n",
        "        images_batch.append(image_data.reshape(p_image_height, p_image_width))\n",
        "\n",
        "    test_images_processed = process_images(np.array(images_batch), p_image_width, p_image_height)\n",
        "\n",
        "    return test_images_processed  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89xE2mY9t4vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(p_image_width, p_image_height, p_num_classes) :\n",
        "    \"\"\"\n",
        "    Creates the compiled model for image classification.\n",
        "\n",
        "    @params:\n",
        "        p_image_width - Required: the image width (int)\n",
        "        p_image_height - Required: the image height (int)\n",
        "        p_num_classes - Required: the number of classes\n",
        "\n",
        "    @returns:\n",
        "      The created and compiled model (Model)        \n",
        "    \"\"\"\n",
        "    \n",
        "    input_shape = (p_image_height, p_image_width, 1)\n",
        "\n",
        "    #we will use a sequential model for training \n",
        "    model = Sequential()\n",
        "    #CONV 3x3x32 => RELU => NORMALIZATION => MAX POOL 3x3 block\n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    #model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "    model.add(Dropout(0.25))\n",
        "    \n",
        "\n",
        "    #CONV 3x3x64 => RELU => NORMALIZATION => MAX POOL 2x2 block\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    #model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    #CONV 3x3x128 => RELU => NORMALIZATION => MAX POOL 2x2 block\n",
        "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    #kernel_regularizer=regularizers.l2(300)\n",
        "    #model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "     #CONV 3x3x256 => RELU => NORMALIZATION => MAX POOL 2x2 added on 12/08\n",
        "    #model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
        "    #model.add(Activation(\"relu\"))\n",
        "    #model.add(ZeroPadding2D((1,1)))\n",
        "    #model.add(BatchNormalization(axis=-1))\n",
        "    #model.add(MaxPooling2D((2, 2), strides=(2,2)))\n",
        "    #model.add(Dropout(0.25))\n",
        "\n",
        "    #FLATTEN => DENSE 1024 => RELU => NORMALIZATION block\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    #final DENSE => SOFTMAX block for multi-label classification\n",
        "    model.add(Dense(p_num_classes))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    #using categorical_crossentropy loss function with adam optimizer\n",
        "    #model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"]) \n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hGg6vmat99A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(\n",
        "    p_model, \n",
        "    p_training_image_data, \n",
        "    p_trainging_labels, \n",
        "    p_batch_size = 32, \n",
        "    p_epochs_to_train = 80, \n",
        "    p_verbose_level = 2\n",
        ") :\n",
        "    \"\"\"\n",
        "    Trains the model using the train image data and train labels.\n",
        "    \n",
        "    @parameters:\n",
        "      p_model - Required: the Keras model to be trained (Model)\n",
        "      p_training_image_data - Required: the image data used for training (array)\n",
        "      p_training_labels - Required: the training labels used fo training (array)\n",
        "      p_batch_size - Optional, default 32: the batch size used for training (int)\n",
        "      p_epochs_to_train - Optional, default 50: number of training epochs (int)\n",
        "      p_verbose_level - Optional, default 2: the Keras verbose level (int)\n",
        "    \n",
        "    @returns:\n",
        "      The trained model (Model)\n",
        "    \"\"\"    \n",
        "    p_model.fit(\n",
        "        x = p_training_image_data, \n",
        "        y = p_trainging_labels, \n",
        "        batch_size = p_batch_size, \n",
        "        epochs = p_epochs_to_train,\n",
        "        shuffle = True,\n",
        "        verbose = p_verbose_level    \n",
        "    )\n",
        "    \n",
        "    return p_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OImQ4Iiqujuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_labels(p_model, p_test_image_data, p_batch_size = 32) :\n",
        "    \"\"\"\n",
        "    Predicts the labels associated with the test data.\n",
        "    \n",
        "    @parameters:\n",
        "      p_model - Required: the Keras model to be used (Model)\n",
        "      p_test_image_data - Required: the image data used for testing (array)\n",
        "      p_batch_size - Optional, default 32: the batch size used for training (int)\n",
        "    \n",
        "    @returns:\n",
        "      The predicted label (array)\n",
        "    \"\"\"      \n",
        "    labels = p_model.predict_classes(p_test_image_data, p_batch_size)\n",
        "  \n",
        "    return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl1_Cjv9urko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_results(\n",
        "    p_testing_submission_file_path, \n",
        "    p_submission_results_file_path, \n",
        "    p_results\n",
        ") :\n",
        "    \"\"\"\n",
        "    Writes the result to the output file.\n",
        "    \n",
        "    @parameters:\n",
        "      p_testing_submission_file_path - Required: the path to the submission format (String)\n",
        "      p_submission_results_file_path - Required: the path to the output file (String)\n",
        "      p_results - Required: the results to be written in the outut file (array)\n",
        "    \"\"\"     \n",
        "    submission_structure = pd.read_csv(p_testing_submission_file_path)\n",
        "    submission_structure['appliance'] = p_results\n",
        "    submission_structure.to_csv(p_submission_results_file_path, index=False)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzQV4LfDuuxB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c9013b1-6b21-4c10-967e-3fbccd22baeb"
      },
      "source": [
        "def main():\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    \n",
        "    #prepare training data\n",
        "    logging.info('Reading training data ...')\n",
        "    train_labels, train_images = generate_train_set(\n",
        "        training_image_data_file_path, \n",
        "        training_labels_data_file_path, \n",
        "        training_image_count, \n",
        "        image_width, \n",
        "        image_height\n",
        "    )\n",
        "    logging.info('Reading training data DONE')\n",
        "    \n",
        "    #create and train model\n",
        "    logging.info('Creating model ...')\n",
        "    model = create_model (image_width, image_height, classes_count)\n",
        "    logging.info('Creating model DONE')\n",
        "\n",
        "    logging.info('Training model ... ')\n",
        "    model = train_model(model, train_images, train_labels, p_epochs_to_train = 80)\n",
        "    logging.info('Training model DONE')\n",
        "    \n",
        "    #create test data\n",
        "    logging.info('Reading testing data ...')\n",
        "    test_images = generate_test_set(\n",
        "      testing_data_file_path, \n",
        "      testing_image_count, \n",
        "      image_width, \n",
        "      image_height\n",
        "    )\n",
        "    logging.info('Reading testing data DONE')\n",
        "    \n",
        "    #predict labels for test data\n",
        "    logging.info('Predicting test data classes ...')\n",
        "    result = predict_labels(model, test_images)\n",
        "    logging.info('Predicting test data classes DONE')\n",
        "    \n",
        "    #write results\n",
        "    logging.info('Writing results ...')\n",
        "    write_results(\n",
        "        testing_submission_file_path, \n",
        "        submission_results_file_path, \n",
        "        result\n",
        "    )\n",
        "    logging.info('Writing results DONE')\n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:Reading training data ...\n",
            "INFO:root:Reading training data DONE\n",
            "INFO:root:Creating model ...\n",
            "INFO:root:Creating model DONE\n",
            "INFO:root:Training model ... \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_69 (Conv2D)           (None, 128, 176, 32)      320       \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 128, 176, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_69 (MaxPooling (None, 42, 58, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 42, 58, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_70 (Conv2D)           (None, 42, 58, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 42, 58, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_70 (MaxPooling (None, 21, 29, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 21, 29, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_71 (Conv2D)           (None, 21, 29, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 21, 29, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_71 (MaxPooling (None, 10, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_55 (Dropout)         (None, 10, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 17920)             0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1024)              18351104  \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 11)                11275     \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 11)                0         \n",
            "=================================================================\n",
            "Total params: 18,455,051\n",
            "Trainable params: 18,455,051\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/80\n",
            " - 23s - loss: 2.8627 - acc: 0.2049\n",
            "Epoch 2/80\n",
            " - 20s - loss: 2.2606 - acc: 0.2118\n",
            "Epoch 3/80\n",
            " - 20s - loss: 2.2364 - acc: 0.2205\n",
            "Epoch 4/80\n",
            " - 20s - loss: 2.2245 - acc: 0.2083\n",
            "Epoch 5/80\n",
            " - 20s - loss: 2.2531 - acc: 0.2031\n",
            "Epoch 6/80\n",
            " - 20s - loss: 2.2074 - acc: 0.2101\n",
            "Epoch 7/80\n",
            " - 20s - loss: 2.2143 - acc: 0.2309\n",
            "Epoch 8/80\n",
            " - 20s - loss: 2.2106 - acc: 0.2049\n",
            "Epoch 9/80\n",
            " - 20s - loss: 2.1973 - acc: 0.2066\n",
            "Epoch 10/80\n",
            " - 20s - loss: 2.2020 - acc: 0.1997\n",
            "Epoch 11/80\n",
            " - 20s - loss: 2.1775 - acc: 0.2257\n",
            "Epoch 12/80\n",
            " - 22s - loss: 2.1869 - acc: 0.2014\n",
            "Epoch 13/80\n",
            " - 21s - loss: 2.1400 - acc: 0.2240\n",
            "Epoch 14/80\n",
            " - 20s - loss: 2.1205 - acc: 0.2257\n",
            "Epoch 15/80\n",
            " - 20s - loss: 2.1371 - acc: 0.2569\n",
            "Epoch 16/80\n",
            " - 20s - loss: 2.0584 - acc: 0.2413\n",
            "Epoch 17/80\n",
            " - 20s - loss: 2.1260 - acc: 0.2569\n",
            "Epoch 18/80\n",
            " - 22s - loss: 2.0184 - acc: 0.2569\n",
            "Epoch 19/80\n",
            " - 20s - loss: 2.0025 - acc: 0.2795\n",
            "Epoch 20/80\n",
            " - 20s - loss: 1.9764 - acc: 0.2882\n",
            "Epoch 21/80\n",
            " - 20s - loss: 2.0238 - acc: 0.2986\n",
            "Epoch 22/80\n",
            " - 21s - loss: 1.9220 - acc: 0.2917\n",
            "Epoch 23/80\n",
            " - 22s - loss: 1.9576 - acc: 0.3247\n",
            "Epoch 24/80\n",
            " - 20s - loss: 1.8511 - acc: 0.3420\n",
            "Epoch 25/80\n",
            " - 22s - loss: 1.8763 - acc: 0.3507\n",
            "Epoch 26/80\n",
            " - 21s - loss: 1.8702 - acc: 0.3351\n",
            "Epoch 27/80\n",
            " - 21s - loss: 1.8341 - acc: 0.3750\n",
            "Epoch 28/80\n",
            " - 20s - loss: 1.7881 - acc: 0.3663\n",
            "Epoch 29/80\n",
            " - 20s - loss: 1.8397 - acc: 0.3698\n",
            "Epoch 30/80\n",
            " - 21s - loss: 1.7601 - acc: 0.3872\n",
            "Epoch 31/80\n",
            " - 20s - loss: 1.7044 - acc: 0.3889\n",
            "Epoch 32/80\n",
            " - 21s - loss: 1.7571 - acc: 0.3854\n",
            "Epoch 33/80\n",
            " - 20s - loss: 1.6970 - acc: 0.4410\n",
            "Epoch 34/80\n",
            " - 21s - loss: 1.6967 - acc: 0.4479\n",
            "Epoch 35/80\n",
            " - 20s - loss: 1.6206 - acc: 0.4618\n",
            "Epoch 36/80\n",
            " - 21s - loss: 1.6519 - acc: 0.4618\n",
            "Epoch 37/80\n",
            " - 21s - loss: 1.6187 - acc: 0.4601\n",
            "Epoch 38/80\n",
            " - 21s - loss: 1.5713 - acc: 0.4844\n",
            "Epoch 39/80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUFHraBLuzE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "from google.colab import files\n",
        "\n",
        "files.download('./data/submission_results.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eSWB_eI8LQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #plot data\n",
        "    plt.plot(model['train_model'], label='Training')\n",
        "    plt.plot(test_images['generate_test_set'], label='Testing')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('iteration ')  \n",
        "    plt.title('training loss iterations')\n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}